package io.github.exploiting.flinks.stateful.operators.advanced_fc

import io.github.exploiting.flinks.stateful.operators.magic.Mappable._
import io.github.exploiting.flinks.stateful.operators.model._
import io.github.exploiting.flinks.stateful.operators.{SimpleTransactionSource, ThrottlingSource}
import org.apache.flink.api.common.state._
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.co.RichCoFlatMapFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

import scala.tools.nsc.interpreter

object AdvancedStatefulFlyingCarpetJob {
  // The source, at a rate of 10 elements/sec.
  val simpleTransactionSource = new ThrottlingSource[SimpleTransaction](new SimpleTransactionSource, 1)

  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)

    val inputStream: DataStream[SimpleTransaction] = env.addSource(simpleTransactionSource)
    val result: DataStream[ScoringResult] = createJob(inputStream)

    result.print()

    println(env.getExecutionPlan)

    // execute program
    env.execute("Flink Advanced-keyed Fraudulent Transaction")
  }

  def createJob(inputStream: DataStream[SimpleTransaction]): DataStream[ScoringResult] = {
    // Give unique Id to incoming event for joining later in the chain.
    val uuidStream: DataStream[UuidTransaction] = inputStream.map { simpleTransaction =>
      val expectedAmountOfFeatures = Manager.definedKeys.size
      UuidTransaction(java.util.UUID.randomUUID, expectedAmountOfFeatures, simpleTransaction.toMap)
    }

    // Determine which features we need to calculate for which key
    val artificialKeyStream: DataStream[ArtificialKeyEvent] = uuidStream.flatMap[ArtificialKeyEvent] {
      (transaction: UuidTransaction, coll: Collector[ArtificialKeyEvent]) =>
        Manager.definedKeys.foreach { key =>
          val keyValue = key.getValue(transaction.payload)

          //minimize the load on the system
          val featureToCalculate = Manager.featureForKey(key)
          val requiredPayload = featureToCalculate.filterRequiredInput(transaction.payload)

          coll.collect(
            ArtificialKeyEvent(transaction.uuid, featureToCalculate.featureName, keyValue, transaction.expectedNumberOfFeatures, requiredPayload)
          )
        }
    }

    // Perform operation at the node where the key belongs.
    val featuresStream: DataStream[FeatureEvent] = artificialKeyStream
        .keyBy(_.artificialKey)
        .mapWithState[FeatureEvent, Map[String, Value]] {
      (event: ArtificialKeyEvent, maybeState: Option[Map[String, Value]]) => {
        val featureToCalculate = Manager.featureForKey(event.artificialKey.map(_._1))
        val (featureResult, newState) = featureToCalculate.calculate(maybeState, event)

        // Return the result, and assign the new state to this operator.
        val featureEvent = FeatureEvent(event.uuid, event.featureName, featureResult, event.expectedNumberOfKeys)
        (featureEvent, Some(newState))
      }
    }

    // Join the original event with the picked up state based on UUID.
    val enrichedStream: DataStream[EnrichedTransaction] = uuidStream.keyBy(_.uuid)
        .connect(featuresStream.keyBy(_.uuid))
        .flatMap(new JoinStreams)

    //now we can score the improved model
    val result = enrichedStream.map(event =>
      ScoringResult(event, FraudRules.scoreAdvancedFlyingCarpet(event))
    )

    result
  }

}

class JoinStreams extends RichCoFlatMapFunction[UuidTransaction, FeatureEvent, EnrichedTransaction] {

  import scala.collection.JavaConverters._

  private var featureEvents: MapState[String, Any] = _
  private var originalEvent: ValueState[UuidTransaction] = _

  private def resultEventsAsScalaMap: Map[String, Any] = Option(featureEvents.entries()).fold(Map.empty[String, Any])(_.iterator().asScala.toList.map(entry => entry.getKey -> entry.getValue).toMap)

  private def numberOfResultEvents: Int = featureEvents.keys().asScala.size

  private def getOriginalEvent: UuidTransaction = Option(originalEvent.value()).get

  private def originalEventAlreadyArrived: Boolean = Option(originalEvent.value()).isDefined

  override def open(config: Configuration): Unit = {
    super.open(config)
    val resultEventDesc: MapStateDescriptor[String, Any] = new MapStateDescriptor("featureEvents", classOf[String], classOf[Any])
    featureEvents = getRuntimeContext.getMapState(resultEventDesc)

    val uuidDesc: ValueStateDescriptor[UuidTransaction] = new ValueStateDescriptor("uuidDescriptor", classOf[UuidTransaction])
    originalEvent = getRuntimeContext.getState(uuidDesc)
  }

  /**
    * What do we do, when an UUIDtransaction comes in?
    * First, check if there are already resultevents, (are we complete? => collect) otherwise update state and continue.
    */
  override def flatMap1(value: UuidTransaction, out: Collector[EnrichedTransaction]): Unit = {
    if (numberOfResultEvents == value.expectedNumberOfFeatures) {
      // We're complete, collect.
      out.collect(EnrichedTransaction(value.uuid, value.payload, resultEventsAsScalaMap))

      featureEvents.clear()
      originalEvent.clear()
    }
    else {
      originalEvent.update(value)
    }
  }

  override def flatMap2(value: FeatureEvent, out: Collector[EnrichedTransaction]): Unit = {

    if (originalEventAlreadyArrived && (numberOfResultEvents + 1 == value.expectedNumberOfFeatures)) {
      // We're complete, collect.
      val originalEventFromState = getOriginalEvent
      out.collect(
        EnrichedTransaction(
          originalEventFromState.uuid,
          originalEventFromState.payload,
          resultEventsAsScalaMap + (value.featureName -> value.result)) // append the new element to the results.
      )

      featureEvents.clear()
      originalEvent.clear()
    }
    else {
      // we're not complete yet. update state.
      featureEvents.put(value.featureName, value.result)
    }
  }
}
